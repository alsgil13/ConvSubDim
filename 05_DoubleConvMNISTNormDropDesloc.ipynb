{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Softmax\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Reshape, GlobalAveragePooling2D, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import RandomNormal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "num_classes = 10 #pegar automaticamente NumPy.Unique\n",
    "epochs = 200\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Cifar-10 Data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "#Adicionando Canal do DataSet (Channel do MNIST eh 1)\n",
    "x_train = np.reshape(x_train, (len(x_train),28,28, 1))\n",
    "x_test = np.reshape(x_test, (len(x_test),28,28, 1))\n",
    "\n",
    "\n",
    "#x_train.shape\n",
    "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
    "#channel=1\n",
    "\n",
    "x_train.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#\n",
    "x_train.shape\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to one hot encoing \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#Filters = IMG_width/2 * 7\n",
    "model.add(Conv2D(98, #alterado de 168 para 98\n",
    "                 (5,5), #alterado de 3 para 5 \n",
    "                 strides=(2,2), \n",
    "                 activation='relu', \n",
    "                 padding='same',\n",
    "                 data_format='channels_last',\n",
    "                 use_bias=True, \n",
    "                 kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=None), \n",
    "                 bias_initializer='zeros',\n",
    "                 input_shape=(img_height, img_width, channel)))\n",
    "\n",
    "#Normalization:\n",
    "#calculo da quantidade de filtros da normalização:\n",
    "# Width*heigth da saída após a primeira camada de convolução+pooling\n",
    "model.add(Conv2D(49, \n",
    "                 (1,1), \n",
    "                 strides=(1,1), \n",
    "                 activation='relu', \n",
    "                 padding='same',\n",
    "                 kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=None), \n",
    "                 data_format='channels_last'))\n",
    "model.add(Conv2D(49, \n",
    "                 (1,1), \n",
    "                 strides=(1,1), \n",
    "                 activation='relu', \n",
    "                 padding='same',\n",
    "                 kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=None),                  \n",
    "                 data_format='channels_last'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), \n",
    "                       strides=(2,2)))\n",
    "\n",
    "\n",
    "\n",
    "#Filters = IMG_width/2 * 9\n",
    "model.add(Conv2D(126, \n",
    "                 (5,5), \n",
    "                 strides=(2,2), \n",
    "                 activation='relu', \n",
    "                 padding='same',\n",
    "                 data_format='channels_last',\n",
    "                 use_bias=True, \n",
    "                 kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=None), \n",
    "                 bias_initializer='zeros',\n",
    "                 input_shape=(img_height, img_width, channel)))\n",
    "#Normalization:\n",
    "#calculo da quantidade de filtros da normalização:\n",
    "# Width*heigth da saída após a primeira camada de convolução+pooling\n",
    "model.add(Conv2D(49, \n",
    "                 (1,1), \n",
    "                 strides=(1,1), \n",
    "                 activation='relu', \n",
    "                 padding='same',\n",
    "                 kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=None),                  \n",
    "                 data_format='channels_last'))\n",
    "model.add(Conv2D(49, \n",
    "                 (1,1), \n",
    "                 strides=(1,1), \n",
    "                 activation='relu', \n",
    "                 padding='same',\n",
    "                 kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=None),                  \n",
    "                 data_format='channels_last'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), \n",
    "                       strides=(2,2)))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "#IMG_width^ 2\n",
    "model.add(Dense(784, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#camada anterior/2\n",
    "model.add(Dense(392, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#Num de classes\n",
    "model.add(Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 98)        2548      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 49)        4851      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 49)        2450      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 49)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 49)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 126)         154476    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 4, 49)          6223      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 49)          2450      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 49)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 49)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 196)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 784)               154448    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 392)               307720    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 392)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                3930      \n",
      "=================================================================\n",
      "Total params: 639,096\n",
      "Trainable params: 639,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print the model summary\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr=0.01, momentum=0.02, decay=0.0, nesterov=False),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "60000/60000 [==============================] - 56s 928us/step - loss: 1.3739 - acc: 0.5081 - val_loss: 0.5017 - val_acc: 0.8889\n",
      "Epoch 2/200\n",
      "60000/60000 [==============================] - 97s 2ms/step - loss: 0.3316 - acc: 0.9003 - val_loss: 0.1807 - val_acc: 0.9642\n",
      "Epoch 3/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.2219 - acc: 0.9351 - val_loss: 0.1250 - val_acc: 0.9740\n",
      "Epoch 4/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.1712 - acc: 0.9497 - val_loss: 0.1056 - val_acc: 0.9771\n",
      "Epoch 5/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.1467 - acc: 0.9571 - val_loss: 0.1647 - val_acc: 0.9685\n",
      "Epoch 6/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.1279 - acc: 0.9628 - val_loss: 0.0808 - val_acc: 0.9820\n",
      "Epoch 7/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.1158 - acc: 0.9664 - val_loss: 0.0763 - val_acc: 0.9841\n",
      "Epoch 8/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.1097 - acc: 0.9691 - val_loss: 0.0759 - val_acc: 0.9835\n",
      "Epoch 9/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0992 - acc: 0.9712 - val_loss: 0.0615 - val_acc: 0.9866\n",
      "Epoch 10/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0955 - acc: 0.9723 - val_loss: 0.0790 - val_acc: 0.9845\n",
      "Epoch 11/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0884 - acc: 0.9748 - val_loss: 0.0747 - val_acc: 0.9851\n",
      "Epoch 12/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0839 - acc: 0.9759 - val_loss: 0.0722 - val_acc: 0.9845\n",
      "Epoch 13/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0803 - acc: 0.9766 - val_loss: 0.0731 - val_acc: 0.9849\n",
      "Epoch 14/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0760 - acc: 0.9782 - val_loss: 0.0564 - val_acc: 0.9876\n",
      "Epoch 15/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0746 - acc: 0.9782 - val_loss: 0.0523 - val_acc: 0.9892\n",
      "Epoch 16/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0701 - acc: 0.9799 - val_loss: 0.0498 - val_acc: 0.9907\n",
      "Epoch 17/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0678 - acc: 0.9805 - val_loss: 0.0526 - val_acc: 0.9906\n",
      "Epoch 18/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0651 - acc: 0.9809 - val_loss: 0.0461 - val_acc: 0.9909\n",
      "Epoch 19/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0630 - acc: 0.9817 - val_loss: 0.0472 - val_acc: 0.9908\n",
      "Epoch 20/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0633 - acc: 0.9816 - val_loss: 0.0423 - val_acc: 0.9915\n",
      "Epoch 21/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0594 - acc: 0.9832 - val_loss: 0.0582 - val_acc: 0.9898\n",
      "Epoch 22/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0557 - acc: 0.9831 - val_loss: 0.0460 - val_acc: 0.9904\n",
      "Epoch 23/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0546 - acc: 0.9842 - val_loss: 0.0453 - val_acc: 0.9905\n",
      "Epoch 24/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0548 - acc: 0.9841 - val_loss: 0.0399 - val_acc: 0.9913\n",
      "Epoch 25/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0507 - acc: 0.9853 - val_loss: 0.0424 - val_acc: 0.9919\n",
      "Epoch 26/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0517 - acc: 0.9849 - val_loss: 0.0449 - val_acc: 0.9921\n",
      "Epoch 27/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0507 - acc: 0.9853 - val_loss: 0.0420 - val_acc: 0.9923\n",
      "Epoch 28/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0486 - acc: 0.9864 - val_loss: 0.0452 - val_acc: 0.9914\n",
      "Epoch 29/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0487 - acc: 0.9862 - val_loss: 0.0418 - val_acc: 0.9904\n",
      "Epoch 30/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0471 - acc: 0.9860 - val_loss: 0.0347 - val_acc: 0.9929\n",
      "Epoch 31/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0460 - acc: 0.9867 - val_loss: 0.0408 - val_acc: 0.9917\n",
      "Epoch 32/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0453 - acc: 0.9864 - val_loss: 0.0387 - val_acc: 0.9909\n",
      "Epoch 33/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0450 - acc: 0.9863 - val_loss: 0.0390 - val_acc: 0.9915\n",
      "Epoch 34/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0430 - acc: 0.9874 - val_loss: 0.0353 - val_acc: 0.9922\n",
      "Epoch 35/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0427 - acc: 0.9881 - val_loss: 0.0346 - val_acc: 0.9926\n",
      "Epoch 36/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0413 - acc: 0.9877 - val_loss: 0.0328 - val_acc: 0.9927\n",
      "Epoch 37/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0388 - acc: 0.9887 - val_loss: 0.0378 - val_acc: 0.9923\n",
      "Epoch 38/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0408 - acc: 0.9886 - val_loss: 0.0397 - val_acc: 0.9920\n",
      "Epoch 39/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0396 - acc: 0.9879 - val_loss: 0.0532 - val_acc: 0.9891\n",
      "Epoch 40/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0409 - acc: 0.9886 - val_loss: 0.0434 - val_acc: 0.9916\n",
      "Epoch 41/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0376 - acc: 0.9892 - val_loss: 0.0314 - val_acc: 0.9925\n",
      "Epoch 42/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0367 - acc: 0.9895 - val_loss: 0.0317 - val_acc: 0.9929\n",
      "Epoch 43/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0396 - acc: 0.9884 - val_loss: 0.0411 - val_acc: 0.9924\n",
      "Epoch 44/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0381 - acc: 0.9889 - val_loss: 0.0341 - val_acc: 0.9916\n",
      "Epoch 45/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0361 - acc: 0.9892 - val_loss: 0.0317 - val_acc: 0.9921\n",
      "Epoch 46/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0349 - acc: 0.9895 - val_loss: 0.0318 - val_acc: 0.9930\n",
      "Epoch 47/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0339 - acc: 0.9897 - val_loss: 0.0327 - val_acc: 0.9927\n",
      "Epoch 48/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0332 - acc: 0.9904 - val_loss: 0.0300 - val_acc: 0.9927\n",
      "Epoch 49/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0336 - acc: 0.9899 - val_loss: 0.0347 - val_acc: 0.9929\n",
      "Epoch 50/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0329 - acc: 0.9906 - val_loss: 0.0427 - val_acc: 0.9901\n",
      "Epoch 51/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0326 - acc: 0.9907 - val_loss: 0.0290 - val_acc: 0.9931\n",
      "Epoch 52/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0321 - acc: 0.9905 - val_loss: 0.0314 - val_acc: 0.9932\n",
      "Epoch 53/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0324 - acc: 0.9904 - val_loss: 0.0378 - val_acc: 0.9910\n",
      "Epoch 54/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0312 - acc: 0.9908 - val_loss: 0.0285 - val_acc: 0.9930\n",
      "Epoch 55/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0304 - acc: 0.9912 - val_loss: 0.0287 - val_acc: 0.9929\n",
      "Epoch 56/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0305 - acc: 0.9910 - val_loss: 0.0312 - val_acc: 0.9931\n",
      "Epoch 57/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0315 - acc: 0.9906 - val_loss: 0.0326 - val_acc: 0.9930\n",
      "Epoch 58/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0306 - acc: 0.9908 - val_loss: 0.0262 - val_acc: 0.9930\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0293 - acc: 0.9912 - val_loss: 0.0323 - val_acc: 0.9920\n",
      "Epoch 60/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0299 - acc: 0.9909 - val_loss: 0.0321 - val_acc: 0.9921\n",
      "Epoch 61/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0281 - acc: 0.9914 - val_loss: 0.0292 - val_acc: 0.9930\n",
      "Epoch 62/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0296 - acc: 0.9915 - val_loss: 0.0293 - val_acc: 0.9925\n",
      "Epoch 63/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0270 - acc: 0.9916 - val_loss: 0.0304 - val_acc: 0.9923\n",
      "Epoch 64/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0259 - acc: 0.9925 - val_loss: 0.0274 - val_acc: 0.9932\n",
      "Epoch 65/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0273 - acc: 0.9915 - val_loss: 0.0315 - val_acc: 0.9927\n",
      "Epoch 66/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0254 - acc: 0.9920 - val_loss: 0.0351 - val_acc: 0.9928\n",
      "Epoch 67/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0258 - acc: 0.9922 - val_loss: 0.0306 - val_acc: 0.9917\n",
      "Epoch 68/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0250 - acc: 0.9925 - val_loss: 0.0272 - val_acc: 0.9933\n",
      "Epoch 69/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0275 - acc: 0.9913 - val_loss: 0.0271 - val_acc: 0.9930\n",
      "Epoch 70/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0245 - acc: 0.9922 - val_loss: 0.0286 - val_acc: 0.9923\n",
      "Epoch 71/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0250 - acc: 0.9920 - val_loss: 0.0283 - val_acc: 0.9932\n",
      "Epoch 72/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0252 - acc: 0.9925 - val_loss: 0.0275 - val_acc: 0.9928\n",
      "Epoch 73/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0260 - acc: 0.9922 - val_loss: 0.0266 - val_acc: 0.9929\n",
      "Epoch 74/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0249 - acc: 0.9927 - val_loss: 0.0276 - val_acc: 0.9929\n",
      "Epoch 75/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0240 - acc: 0.9929 - val_loss: 0.0297 - val_acc: 0.9927\n",
      "Epoch 76/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0234 - acc: 0.9929 - val_loss: 0.0248 - val_acc: 0.9936\n",
      "Epoch 77/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0236 - acc: 0.9929 - val_loss: 0.0280 - val_acc: 0.9934\n",
      "Epoch 78/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0226 - acc: 0.9935 - val_loss: 0.0277 - val_acc: 0.9926\n",
      "Epoch 79/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0217 - acc: 0.9930 - val_loss: 0.0258 - val_acc: 0.9932\n",
      "Epoch 80/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0215 - acc: 0.9932 - val_loss: 0.0272 - val_acc: 0.9929\n",
      "Epoch 81/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0256 - acc: 0.9923 - val_loss: 0.0291 - val_acc: 0.9925\n",
      "Epoch 82/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0220 - acc: 0.9934 - val_loss: 0.0248 - val_acc: 0.9932\n",
      "Epoch 83/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0237 - acc: 0.9931 - val_loss: 0.0285 - val_acc: 0.9935\n",
      "Epoch 84/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0220 - acc: 0.9932 - val_loss: 0.0286 - val_acc: 0.9932\n",
      "Epoch 85/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0220 - acc: 0.9933 - val_loss: 0.0238 - val_acc: 0.9936\n",
      "Epoch 86/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0214 - acc: 0.9936 - val_loss: 0.0276 - val_acc: 0.9931\n",
      "Epoch 87/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0212 - acc: 0.9933 - val_loss: 0.0268 - val_acc: 0.9930\n",
      "Epoch 88/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0219 - acc: 0.9931 - val_loss: 0.0296 - val_acc: 0.9926\n",
      "Epoch 89/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0228 - acc: 0.9935 - val_loss: 0.0272 - val_acc: 0.9929\n",
      "Epoch 90/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0206 - acc: 0.9939 - val_loss: 0.0264 - val_acc: 0.9940\n",
      "Epoch 91/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0224 - acc: 0.9934 - val_loss: 0.0254 - val_acc: 0.9935\n",
      "Epoch 92/200\n",
      "60000/60000 [==============================] - 98s 2ms/step - loss: 0.0179 - acc: 0.9944 - val_loss: 0.0273 - val_acc: 0.9924\n",
      "Epoch 93/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0214 - acc: 0.9934 - val_loss: 0.0270 - val_acc: 0.9930\n",
      "Epoch 94/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0181 - acc: 0.9944 - val_loss: 0.0246 - val_acc: 0.9925\n",
      "Epoch 95/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0199 - acc: 0.9937 - val_loss: 0.0243 - val_acc: 0.9934\n",
      "Epoch 96/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0191 - acc: 0.9943 - val_loss: 0.0243 - val_acc: 0.9935\n",
      "Epoch 97/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0180 - acc: 0.9945 - val_loss: 0.0262 - val_acc: 0.9925\n",
      "Epoch 98/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0203 - acc: 0.9939 - val_loss: 0.0252 - val_acc: 0.9935\n",
      "Epoch 99/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0179 - acc: 0.9947 - val_loss: 0.0246 - val_acc: 0.9924\n",
      "Epoch 100/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0200 - acc: 0.9940 - val_loss: 0.0240 - val_acc: 0.9939\n",
      "Epoch 101/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0203 - acc: 0.9940 - val_loss: 0.0299 - val_acc: 0.9923\n",
      "Epoch 102/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0176 - acc: 0.9946 - val_loss: 0.0264 - val_acc: 0.9922\n",
      "Epoch 103/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0183 - acc: 0.9944 - val_loss: 0.0231 - val_acc: 0.9939\n",
      "Epoch 104/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0175 - acc: 0.9945 - val_loss: 0.0226 - val_acc: 0.9935\n",
      "Epoch 105/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0183 - acc: 0.9945 - val_loss: 0.0249 - val_acc: 0.9928\n",
      "Epoch 106/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0165 - acc: 0.9948 - val_loss: 0.0239 - val_acc: 0.9936\n",
      "Epoch 107/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0170 - acc: 0.9951 - val_loss: 0.0277 - val_acc: 0.9926\n",
      "Epoch 108/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0177 - acc: 0.9947 - val_loss: 0.0241 - val_acc: 0.9932\n",
      "Epoch 109/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0163 - acc: 0.9948 - val_loss: 0.0230 - val_acc: 0.9939\n",
      "Epoch 110/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0179 - acc: 0.9945 - val_loss: 0.0234 - val_acc: 0.9937\n",
      "Epoch 111/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0179 - acc: 0.9947 - val_loss: 0.0262 - val_acc: 0.9934\n",
      "Epoch 112/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0165 - acc: 0.9946 - val_loss: 0.0220 - val_acc: 0.9939\n",
      "Epoch 113/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0173 - acc: 0.9945 - val_loss: 0.0331 - val_acc: 0.9908\n",
      "Epoch 114/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0167 - acc: 0.9949 - val_loss: 0.0225 - val_acc: 0.9936\n",
      "Epoch 115/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0175 - acc: 0.9947 - val_loss: 0.0261 - val_acc: 0.9928\n",
      "Epoch 116/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0170 - acc: 0.9948 - val_loss: 0.0237 - val_acc: 0.9939\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0153 - acc: 0.9951 - val_loss: 0.0215 - val_acc: 0.9939\n",
      "Epoch 118/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0183 - acc: 0.9947 - val_loss: 0.0244 - val_acc: 0.9933\n",
      "Epoch 119/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0179 - acc: 0.9945 - val_loss: 0.0223 - val_acc: 0.9938\n",
      "Epoch 120/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0157 - acc: 0.9954 - val_loss: 0.0231 - val_acc: 0.9937\n",
      "Epoch 121/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0164 - acc: 0.9951 - val_loss: 0.0253 - val_acc: 0.9928\n",
      "Epoch 122/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0149 - acc: 0.9954 - val_loss: 0.0220 - val_acc: 0.9932\n",
      "Epoch 123/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0152 - acc: 0.9953 - val_loss: 0.0212 - val_acc: 0.9937\n",
      "Epoch 124/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0146 - acc: 0.9957 - val_loss: 0.0239 - val_acc: 0.9935\n",
      "Epoch 125/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0159 - acc: 0.9954 - val_loss: 0.0221 - val_acc: 0.9945\n",
      "Epoch 126/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0134 - acc: 0.9959 - val_loss: 0.0247 - val_acc: 0.9936\n",
      "Epoch 127/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0148 - acc: 0.9955 - val_loss: 0.0225 - val_acc: 0.9937\n",
      "Epoch 128/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0156 - acc: 0.9953 - val_loss: 0.0233 - val_acc: 0.9928\n",
      "Epoch 129/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0157 - acc: 0.9952 - val_loss: 0.0217 - val_acc: 0.9937\n",
      "Epoch 130/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0148 - acc: 0.9955 - val_loss: 0.0229 - val_acc: 0.9934\n",
      "Epoch 131/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0141 - acc: 0.9957 - val_loss: 0.0243 - val_acc: 0.9924\n",
      "Epoch 132/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0144 - acc: 0.9951 - val_loss: 0.0249 - val_acc: 0.9923\n",
      "Epoch 133/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0145 - acc: 0.9955 - val_loss: 0.0233 - val_acc: 0.9930\n",
      "Epoch 134/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0147 - acc: 0.9956 - val_loss: 0.0231 - val_acc: 0.9931\n",
      "Epoch 135/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0154 - acc: 0.9956 - val_loss: 0.0218 - val_acc: 0.9938\n",
      "Epoch 136/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0146 - acc: 0.9953 - val_loss: 0.0241 - val_acc: 0.9929\n",
      "Epoch 137/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0142 - acc: 0.9956 - val_loss: 0.0224 - val_acc: 0.9941\n",
      "Epoch 138/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0152 - acc: 0.9954 - val_loss: 0.0234 - val_acc: 0.9939\n",
      "Epoch 139/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0137 - acc: 0.9957 - val_loss: 0.0221 - val_acc: 0.9937\n",
      "Epoch 140/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0121 - acc: 0.9961 - val_loss: 0.0210 - val_acc: 0.9944\n",
      "Epoch 141/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0169 - acc: 0.9950 - val_loss: 0.0250 - val_acc: 0.9935\n",
      "Epoch 142/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0136 - acc: 0.9962 - val_loss: 0.0222 - val_acc: 0.9939\n",
      "Epoch 143/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0129 - acc: 0.9960 - val_loss: 0.0241 - val_acc: 0.9932\n",
      "Epoch 144/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0134 - acc: 0.9961 - val_loss: 0.0241 - val_acc: 0.9924\n",
      "Epoch 145/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0148 - acc: 0.9953 - val_loss: 0.0243 - val_acc: 0.9937\n",
      "Epoch 146/200\n",
      "60000/60000 [==============================] - 97s 2ms/step - loss: 0.0124 - acc: 0.9964 - val_loss: 0.0219 - val_acc: 0.9938\n",
      "Epoch 147/200\n",
      "60000/60000 [==============================] - 98s 2ms/step - loss: 0.0129 - acc: 0.9960 - val_loss: 0.0215 - val_acc: 0.9943\n",
      "Epoch 148/200\n",
      "60000/60000 [==============================] - 98s 2ms/step - loss: 0.0130 - acc: 0.9961 - val_loss: 0.0237 - val_acc: 0.9930\n",
      "Epoch 149/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0115 - acc: 0.9968 - val_loss: 0.0202 - val_acc: 0.9935\n",
      "Epoch 150/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0125 - acc: 0.9960 - val_loss: 0.0197 - val_acc: 0.9946\n",
      "Epoch 151/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0127 - acc: 0.9961 - val_loss: 0.0225 - val_acc: 0.9936\n",
      "Epoch 152/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0147 - acc: 0.9954 - val_loss: 0.0236 - val_acc: 0.9935\n",
      "Epoch 153/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0134 - acc: 0.9960 - val_loss: 0.0214 - val_acc: 0.9940\n",
      "Epoch 154/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0127 - acc: 0.9961 - val_loss: 0.0221 - val_acc: 0.9944\n",
      "Epoch 155/200\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.0114 - acc: 0.9965 - val_loss: 0.0208 - val_acc: 0.9940\n",
      "Epoch 156/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0135 - acc: 0.9961 - val_loss: 0.0227 - val_acc: 0.9938\n",
      "Epoch 157/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0125 - acc: 0.9962 - val_loss: 0.0224 - val_acc: 0.9939\n",
      "Epoch 158/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0104 - acc: 0.9967 - val_loss: 0.0207 - val_acc: 0.9942\n",
      "Epoch 159/200\n",
      "60000/60000 [==============================] - 97s 2ms/step - loss: 0.0115 - acc: 0.9965 - val_loss: 0.0233 - val_acc: 0.9928\n",
      "Epoch 160/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0124 - acc: 0.9962 - val_loss: 0.0268 - val_acc: 0.9919\n",
      "Epoch 161/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0111 - acc: 0.9966 - val_loss: 0.0215 - val_acc: 0.9936\n",
      "Epoch 162/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0115 - acc: 0.9966 - val_loss: 0.0220 - val_acc: 0.9934\n",
      "Epoch 163/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0113 - acc: 0.9966 - val_loss: 0.0245 - val_acc: 0.9932\n",
      "Epoch 164/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0124 - acc: 0.9960 - val_loss: 0.0288 - val_acc: 0.9920\n",
      "Epoch 165/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0123 - acc: 0.9962 - val_loss: 0.0220 - val_acc: 0.9940\n",
      "Epoch 166/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0106 - acc: 0.9967 - val_loss: 0.0226 - val_acc: 0.9930\n",
      "Epoch 167/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0123 - acc: 0.9963 - val_loss: 0.0226 - val_acc: 0.9937\n",
      "Epoch 168/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0129 - acc: 0.9961 - val_loss: 0.0205 - val_acc: 0.9940\n",
      "Epoch 169/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0110 - acc: 0.9966 - val_loss: 0.0227 - val_acc: 0.9935\n",
      "Epoch 170/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0116 - acc: 0.9967 - val_loss: 0.0230 - val_acc: 0.9932\n",
      "Epoch 171/200\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.0109 - acc: 0.9965 - val_loss: 0.0233 - val_acc: 0.9936\n",
      "Epoch 172/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0108 - acc: 0.9963 - val_loss: 0.0239 - val_acc: 0.9930\n",
      "Epoch 173/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0128 - acc: 0.9960 - val_loss: 0.0290 - val_acc: 0.9921\n",
      "Epoch 174/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0111 - acc: 0.9965 - val_loss: 0.0223 - val_acc: 0.9935\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0106 - acc: 0.9967 - val_loss: 0.0208 - val_acc: 0.9939\n",
      "Epoch 176/200\n",
      "60000/60000 [==============================] - 98s 2ms/step - loss: 0.0107 - acc: 0.9967 - val_loss: 0.0213 - val_acc: 0.9941\n",
      "Epoch 177/200\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.0100 - acc: 0.9970 - val_loss: 0.0205 - val_acc: 0.9943\n",
      "Epoch 178/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0109 - acc: 0.9966 - val_loss: 0.0215 - val_acc: 0.9934\n",
      "Epoch 179/200\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.0108 - acc: 0.9964 - val_loss: 0.0222 - val_acc: 0.9935\n",
      "Epoch 180/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0114 - acc: 0.9966 - val_loss: 0.0216 - val_acc: 0.9935\n",
      "Epoch 181/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0110 - acc: 0.9963 - val_loss: 0.0254 - val_acc: 0.9926\n",
      "Epoch 182/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0114 - acc: 0.9966 - val_loss: 0.0216 - val_acc: 0.9935\n",
      "Epoch 183/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0104 - acc: 0.9968 - val_loss: 0.0221 - val_acc: 0.9931\n",
      "Epoch 184/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0102 - acc: 0.9967 - val_loss: 0.0205 - val_acc: 0.9938\n",
      "Epoch 185/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0102 - acc: 0.9969 - val_loss: 0.0215 - val_acc: 0.9932\n",
      "Epoch 186/200\n",
      "60000/60000 [==============================] - 98s 2ms/step - loss: 0.0099 - acc: 0.9968 - val_loss: 0.0214 - val_acc: 0.9934\n",
      "Epoch 187/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0110 - acc: 0.9965 - val_loss: 0.0204 - val_acc: 0.9937\n",
      "Epoch 188/200\n",
      "60000/60000 [==============================] - 98s 2ms/step - loss: 0.0130 - acc: 0.9962 - val_loss: 0.0228 - val_acc: 0.9933\n",
      "Epoch 189/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0106 - acc: 0.9970 - val_loss: 0.0227 - val_acc: 0.9929\n",
      "Epoch 190/200\n",
      "60000/60000 [==============================] - 98s 2ms/step - loss: 0.0092 - acc: 0.9973 - val_loss: 0.0217 - val_acc: 0.9941\n",
      "Epoch 191/200\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0119 - acc: 0.9963 - val_loss: 0.0209 - val_acc: 0.9942\n",
      "Epoch 192/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0098 - acc: 0.9969 - val_loss: 0.0239 - val_acc: 0.9928\n",
      "Epoch 193/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0094 - acc: 0.9970 - val_loss: 0.0223 - val_acc: 0.9933\n",
      "Epoch 194/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0096 - acc: 0.9971 - val_loss: 0.0207 - val_acc: 0.9937\n",
      "Epoch 195/200\n",
      "60000/60000 [==============================] - 98s 2ms/step - loss: 0.0108 - acc: 0.9966 - val_loss: 0.0225 - val_acc: 0.9929\n",
      "Epoch 196/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0103 - acc: 0.9967 - val_loss: 0.0216 - val_acc: 0.9934\n",
      "Epoch 197/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0104 - acc: 0.9970 - val_loss: 0.0221 - val_acc: 0.9932\n",
      "Epoch 198/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0094 - acc: 0.9970 - val_loss: 0.0259 - val_acc: 0.9930\n",
      "Epoch 199/200\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0102 - acc: 0.9969 - val_loss: 0.0248 - val_acc: 0.9927\n",
      "Epoch 200/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0109 - acc: 0.9966 - val_loss: 0.0209 - val_acc: 0.9935\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the Model\n",
    "#earlyStopping=keras.callbacks.EarlyStopping(monitor='val_acc', min_delta=0, patience=13, verbose=1, mode='auto')\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "#                    callbacks=[earlyStopping],\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f81b76f85c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train acc','val acc'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfVfQZ6V/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f81b4472550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train loss','Val Loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.020924600178\n",
      "Test accuracy: 0.9935\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [0.50174464502334593, 0.18071965031623841, 0.12497882691025734, 0.10556106217503548, 0.16470163841247559, 0.080847257126867772, 0.076259505516290668, 0.075876217073202135, 0.061502750273048877, 0.078978660917282104, 0.074748550045490261, 0.072216487932205198, 0.073085328543186182, 0.056354746977239849, 0.05227510609328747, 0.049849504834413527, 0.052635007171332834, 0.04607348812445998, 0.047236074259877206, 0.042258942720293999, 0.058157702922821047, 0.046036343470215797, 0.045270083647966385, 0.03993674049898982, 0.042400337394326928, 0.044863670816272495, 0.041980873441696168, 0.045207618126273157, 0.041758849067986013, 0.034730917849950492, 0.040770934121310713, 0.038665981622040269, 0.039040028953552249, 0.035291366613656283, 0.034562231995165345, 0.032788333548605442, 0.037777480264753104, 0.039719711498171092, 0.05318047375231981, 0.043378429178893563, 0.031352409524843097, 0.031663887315616014, 0.041090355060994628, 0.03409420510344207, 0.031708580187894402, 0.031778422153741125, 0.03268906397558749, 0.030023500063456596, 0.03473746858239174, 0.042652686723694204, 0.029026513524726034, 0.031373137042857706, 0.037790154672786597, 0.028470866286940871, 0.028674195230193437, 0.031196657073311509, 0.032634097847156228, 0.026226194800715894, 0.032344538665004072, 0.032138730165921149, 0.029159681940823794, 0.029251681919209658, 0.030418841397017241, 0.027362148754205555, 0.031479364622756842, 0.035144117650762204, 0.030565467249229549, 0.02716455239970237, 0.02707337090903893, 0.028631041127443312, 0.028256604833016171, 0.027493630367983132, 0.026555024507082997, 0.027602108491864057, 0.029744269177317619, 0.02477942188922316, 0.027974928040616215, 0.027674991038627923, 0.025753061855351551, 0.027179505223967136, 0.029097599329240619, 0.024755800439836457, 0.028455293439794332, 0.028587781324796377, 0.023837134898127988, 0.027630139749031515, 0.026775043234974146, 0.029639780951570719, 0.027224874073453247, 0.026443330179527401, 0.025367702002637087, 0.027275269326940178, 0.027019776999112219, 0.024604867068515158, 0.024311279240762814, 0.024318699278868736, 0.026231416189717128, 0.025186853909282946, 0.024553867728775366, 0.024048817331762985, 0.029948291701637209, 0.026438573641562834, 0.023122690804698504, 0.022557187040662393, 0.024930268181930296, 0.023885004047630356, 0.027670344475237654, 0.024108058893727139, 0.023049669321626425, 0.023428015778400004, 0.02619515063688159, 0.022041978633822873, 0.033072806217521428, 0.022460337219107895, 0.026146854676678776, 0.023743479884043336, 0.021451955320918932, 0.024353833719296381, 0.022320989878242836, 0.023082368561066689, 0.025314496681001036, 0.021996752977836879, 0.021241736113769002, 0.023928770669526419, 0.022077940820902586, 0.024695165755925699, 0.022508826633729041, 0.023302477133087814, 0.021692946884781121, 0.022943256473913789, 0.02431600336721167, 0.024864926784904675, 0.023258712087501771, 0.023107428306224757, 0.02175956664122641, 0.024081302486523053, 0.022355534436949528, 0.023407804762828163, 0.022057056174287572, 0.020962847641506233, 0.025004407803621145, 0.022172567460685969, 0.024118037587963045, 0.024138271466572769, 0.024300639049289748, 0.021924799480452201, 0.021514698994834908, 0.02370997939025983, 0.020225815403216985, 0.019704080529510976, 0.022478495734022001, 0.023607695251726544, 0.021360482439165936, 0.022054573969892226, 0.020847974538104608, 0.022675560806761495, 0.022357688949187285, 0.020685383580345662, 0.023271776721649803, 0.026750906230066904, 0.021489019897254182, 0.022003148960543329, 0.024474516138376201, 0.028772074411949143, 0.022020906468958129, 0.022626849707716612, 0.022590217904141174, 0.020544729311321862, 0.022695859726332129, 0.022980003726342693, 0.023312945555744227, 0.023919985631236342, 0.028975310276355595, 0.022303757258434781, 0.020824495411920361, 0.021306766341248295, 0.020530376607645302, 0.021470199154183502, 0.022161026994162239, 0.021614973781676962, 0.025360620793839918, 0.021598583102499833, 0.022125563098653218, 0.020482163218641655, 0.02152741930517368, 0.021398318924009799, 0.020378092746052425, 0.022826511148130522, 0.022670627687522212, 0.021672071006521583, 0.020926119040686173, 0.023928702776110731, 0.02229585411711596, 0.020690562444523674, 0.022495592725637833, 0.021605273648723961, 0.022065294726938009, 0.025853454576840157, 0.024798817903402961, 0.020924600202683358], 'val_acc': [0.88890000000000002, 0.96419999999999995, 0.97399999999999998, 0.97709999999999997, 0.96850000000000003, 0.98199999999999998, 0.98409999999999997, 0.98350000000000004, 0.98660000000000003, 0.98450000000000004, 0.98509999999999998, 0.98450000000000004, 0.9849, 0.98760000000000003, 0.98919999999999997, 0.99070000000000003, 0.99060000000000004, 0.9909, 0.99080000000000001, 0.99150000000000005, 0.98980000000000001, 0.99039999999999995, 0.99050000000000005, 0.99129999999999996, 0.9919, 0.99209999999999998, 0.99229999999999996, 0.99139999999999995, 0.99039999999999995, 0.9929, 0.99170000000000003, 0.9909, 0.99150000000000005, 0.99219999999999997, 0.99260000000000004, 0.99270000000000003, 0.99229999999999996, 0.99199999999999999, 0.98909999999999998, 0.99160000000000004, 0.99250000000000005, 0.9929, 0.99239999999999995, 0.99160000000000004, 0.99209999999999998, 0.99299999999999999, 0.99270000000000003, 0.99270000000000003, 0.9929, 0.99009999999999998, 0.99309999999999998, 0.99319999999999997, 0.99099999999999999, 0.99299999999999999, 0.9929, 0.99309999999999998, 0.99299999999999999, 0.99299999999999999, 0.99199999999999999, 0.99209999999999998, 0.99299999999999999, 0.99250000000000005, 0.99229999999999996, 0.99319999999999997, 0.99270000000000003, 0.99280000000000002, 0.99170000000000003, 0.99329999999999996, 0.99299999999999999, 0.99229999999999996, 0.99319999999999997, 0.99280000000000002, 0.9929, 0.9929, 0.99270000000000003, 0.99360000000000004, 0.99339999999999995, 0.99260000000000004, 0.99319999999999997, 0.9929, 0.99250000000000005, 0.99319999999999997, 0.99350000000000005, 0.99319999999999997, 0.99360000000000004, 0.99309999999999998, 0.99299999999999999, 0.99260000000000004, 0.9929, 0.99399999999999999, 0.99350000000000005, 0.99239999999999995, 0.99299999999999999, 0.99250000000000005, 0.99339999999999995, 0.99350000000000005, 0.99250000000000005, 0.99350000000000005, 0.99239999999999995, 0.99390000000000001, 0.99229999999999996, 0.99219999999999997, 0.99390000000000001, 0.99350000000000005, 0.99280000000000002, 0.99360000000000004, 0.99260000000000004, 0.99319999999999997, 0.99390000000000001, 0.99370000000000003, 0.99339999999999995, 0.99390000000000001, 0.99080000000000001, 0.99360000000000004, 0.99280000000000002, 0.99390000000000001, 0.99390000000000001, 0.99329999999999996, 0.99380000000000002, 0.99370000000000003, 0.99280000000000002, 0.99319999999999997, 0.99370000000000003, 0.99350000000000005, 0.99450000000000005, 0.99360000000000004, 0.99370000000000003, 0.99280000000000002, 0.99370000000000003, 0.99339999999999995, 0.99239999999999995, 0.99229999999999996, 0.99299999999999999, 0.99309999999999998, 0.99380000000000002, 0.9929, 0.99409999999999998, 0.99390000000000001, 0.99370000000000003, 0.99439999999999995, 0.99350000000000005, 0.99390000000000001, 0.99319999999999997, 0.99239999999999995, 0.99370000000000003, 0.99380000000000002, 0.99429999999999996, 0.99299999999999999, 0.99350000000000005, 0.99460000000000004, 0.99360000000000004, 0.99350000000000005, 0.99399999999999999, 0.99439999999999995, 0.99399999999999999, 0.99380000000000002, 0.99390000000000001, 0.99419999999999997, 0.99280000000000002, 0.9919, 0.99360000000000004, 0.99339999999999995, 0.99319999999999997, 0.99199999999999999, 0.99399999999999999, 0.99299999999999999, 0.99370000000000003, 0.99399999999999999, 0.99350000000000005, 0.99319999999999997, 0.99360000000000004, 0.99299999999999999, 0.99209999999999998, 0.99350000000000005, 0.99390000000000001, 0.99409999999999998, 0.99429999999999996, 0.99339999999999995, 0.99350000000000005, 0.99350000000000005, 0.99260000000000004, 0.99350000000000005, 0.99309999999999998, 0.99380000000000002, 0.99319999999999997, 0.99339999999999995, 0.99370000000000003, 0.99329999999999996, 0.9929, 0.99409999999999998, 0.99419999999999997, 0.99280000000000002, 0.99329999999999996, 0.99370000000000003, 0.9929, 0.99339999999999995, 0.99319999999999997, 0.99299999999999999, 0.99270000000000003, 0.99350000000000005], 'loss': [1.3739215002695719, 0.33159869623978933, 0.22191182833512624, 0.17122391302386919, 0.14669191968739032, 0.12793702198515336, 0.11584116664938629, 0.109717945108066, 0.099206994896133743, 0.095476788300772511, 0.088427641991774247, 0.083880614938835307, 0.080308875721817219, 0.076007991129284108, 0.074616713776066892, 0.07005522540286184, 0.067801607033920783, 0.065067160860821607, 0.062981362593546517, 0.0633133709559838, 0.059444149869680406, 0.055677629331002634, 0.05455689842502276, 0.054800432089654107, 0.050655690639590224, 0.051671270929711563, 0.050728233335167169, 0.048614459087668607, 0.048671791948005555, 0.047117749942094091, 0.046027658584279318, 0.045287112046064192, 0.045037739025801417, 0.042972228481123843, 0.042747775139535468, 0.041319619313658525, 0.038773065852498012, 0.040832410884803785, 0.039555101610720157, 0.040869967461936177, 0.037646391032605121, 0.03666404404255251, 0.039624316265154627, 0.038114672115057084, 0.036063608031657833, 0.03492317303943758, 0.033901368679323543, 0.033227967280871233, 0.033632904942644141, 0.03287598419900363, 0.032579591297451409, 0.032134682221757249, 0.032368195781996477, 0.031244722431463499, 0.030353402919663736, 0.030538309746895295, 0.031546477941796186, 0.030579732694818326, 0.029310837859474123, 0.029930988485614457, 0.028142528039248039, 0.029626384893059729, 0.026967050111511102, 0.025854714102437719, 0.027263855700458712, 0.025360534604522398, 0.025813009331313273, 0.024984004356448229, 0.027517121260089334, 0.024480778136686421, 0.024961332791290865, 0.025193284218401338, 0.02599653420203055, 0.024942682251085837, 0.023967373565118761, 0.023425727305266385, 0.023557507106382401, 0.022554484391377385, 0.021716752459279572, 0.021530224222410469, 0.025631629914557561, 0.021966466829723988, 0.023683348022579837, 0.022037517825568405, 0.021969130492035766, 0.021431528271986949, 0.021199994863336905, 0.021874318252286563, 0.022804702035632604, 0.020554794989634927, 0.022408433488546872, 0.017919720681671365, 0.021369773631185914, 0.018147692311401866, 0.01994195679911645, 0.019116849743027707, 0.01799987342839595, 0.02033566548776192, 0.01785486553561641, 0.020024248751530346, 0.020251037050791395, 0.017598093384189998, 0.018349740125543516, 0.01753742547671621, 0.018332165091713735, 0.016514392958907411, 0.016997461315620847, 0.017700145060136372, 0.0162579164537834, 0.017892135636620999, 0.017876708245952614, 0.016466833711760893, 0.017291344817669595, 0.016747821369383017, 0.017548029235368207, 0.017027117979732186, 0.015266595725150546, 0.018333280531700195, 0.017890589089808055, 0.015667347197630441, 0.016399007585116972, 0.014925628637780512, 0.015199324853456346, 0.014587119285256389, 0.015888489890325583, 0.013387079097412061, 0.014831643572789229, 0.015615209333687865, 0.015653604223323056, 0.014806803990361125, 0.014103571774827529, 0.014375442918436602, 0.014457609561949599, 0.014678435029635147, 0.015375018888430108, 0.01458605299081925, 0.014227091073117723, 0.015172086000395938, 0.0136661601209819, 0.012112269580847351, 0.016905163610477274, 0.013619970325500859, 0.012867330847301734, 0.013393502707356432, 0.01482566148439267, 0.012413317746112443, 0.012854224564509544, 0.013021074092123308, 0.01147139385760626, 0.012465981632531233, 0.012747518884020974, 0.014686565105933308, 0.013374052945621467, 0.012660778359767089, 0.011409857830516314, 0.013462403970741435, 0.012532849249143814, 0.010402615598532429, 0.011467505762550475, 0.012397229901997585, 0.011081823627150152, 0.011466787065722748, 0.011316834371446264, 0.012371599872304554, 0.012337897771232141, 0.010614672122674286, 0.012282434384827018, 0.012883317833635131, 0.011008239523821977, 0.01158243019774903, 0.01090231678262062, 0.010813247275871496, 0.012750576155558762, 0.011080648689251999, 0.010592698282687343, 0.010677903491964874, 0.0099758466863984121, 0.010864734715400845, 0.010754703534402263, 0.011414845210615508, 0.010987153911622105, 0.011399970587479281, 0.010381377420637243, 0.010218982058418624, 0.010159582333455425, 0.0099284519398594659, 0.011004151178391961, 0.012983037716275915, 0.010575657542839083, 0.0091507728461472041, 0.011949791398921418, 0.0097688606727814962, 0.0094029303977597005, 0.0095717561149926047, 0.01077821420527301, 0.010337012216567625, 0.010421567786267164, 0.0093831842008289344, 0.01022016492049758, 0.010859570149228967], 'acc': [0.5081, 0.90031666666666665, 0.93511666666666671, 0.94969999999999999, 0.95706666666666662, 0.96283333333333332, 0.96640000000000004, 0.96913333333333329, 0.97123333333333328, 0.97228333333333339, 0.9748, 0.97593333333333332, 0.97655000000000003, 0.97821666666666662, 0.97823333333333329, 0.97986666666666666, 0.98053333333333337, 0.98091666666666666, 0.98168333333333335, 0.98161666666666669, 0.9831833333333333, 0.98309999999999997, 0.98421666666666663, 0.98409999999999997, 0.98531666666666662, 0.98488333333333333, 0.98529999999999995, 0.98641666666666672, 0.98616666666666664, 0.98599999999999999, 0.98670000000000002, 0.98643333333333338, 0.98631666666666662, 0.98736666666666661, 0.98808333333333331, 0.98773333333333335, 0.98865000000000003, 0.98860000000000003, 0.9879, 0.98860000000000003, 0.9892333333333333, 0.98953333333333338, 0.98843333333333339, 0.9889, 0.98921666666666663, 0.98953333333333338, 0.98971666666666669, 0.99039999999999995, 0.98995, 0.99060000000000004, 0.99065000000000003, 0.99045000000000005, 0.99043333333333339, 0.99078333333333335, 0.99118333333333331, 0.99103333333333332, 0.99055000000000004, 0.99078333333333335, 0.9912333333333333, 0.99086666666666667, 0.99139999999999995, 0.99151666666666671, 0.99160000000000004, 0.99246666666666672, 0.99151666666666671, 0.99201666666666666, 0.99219999999999997, 0.99250000000000005, 0.99131666666666662, 0.99218333333333331, 0.99204999999999999, 0.99250000000000005, 0.9922333333333333, 0.99273333333333336, 0.99291666666666667, 0.99293333333333333, 0.99286666666666668, 0.99346666666666672, 0.99304999999999999, 0.99321666666666664, 0.99233333333333329, 0.99341666666666661, 0.99308333333333332, 0.99319999999999997, 0.9932833333333333, 0.99356666666666671, 0.99334999999999996, 0.99306666666666665, 0.99351666666666671, 0.99391666666666667, 0.99345000000000006, 0.99443333333333328, 0.99339999999999995, 0.99443333333333328, 0.99370000000000003, 0.99433333333333329, 0.99446666666666672, 0.99391666666666667, 0.9946666666666667, 0.99404999999999999, 0.99398333333333333, 0.9946166666666667, 0.99438333333333329, 0.99451666666666672, 0.99448333333333339, 0.99478333333333335, 0.99509999999999998, 0.99473333333333336, 0.99480000000000002, 0.99450000000000005, 0.99468333333333336, 0.99463333333333337, 0.99453333333333338, 0.99485000000000001, 0.99465000000000003, 0.99481666666666668, 0.99514999999999998, 0.9946666666666667, 0.99455000000000005, 0.99543333333333328, 0.99508333333333332, 0.99541666666666662, 0.9952833333333333, 0.99568333333333336, 0.99541666666666662, 0.99590000000000001, 0.99548333333333339, 0.99526666666666663, 0.99516666666666664, 0.99553333333333338, 0.99570000000000003, 0.99514999999999998, 0.99550000000000005, 0.99561666666666671, 0.99555000000000005, 0.99534999999999996, 0.99558333333333338, 0.99536666666666662, 0.99570000000000003, 0.99609999999999999, 0.99496666666666667, 0.99616666666666664, 0.99595, 0.99609999999999999, 0.99526666666666663, 0.99636666666666662, 0.99601666666666666, 0.99609999999999999, 0.99680000000000002, 0.99603333333333333, 0.99613333333333332, 0.99544999999999995, 0.99603333333333333, 0.99614999999999998, 0.99653333333333338, 0.99608333333333332, 0.99616666666666664, 0.99670000000000003, 0.99644999999999995, 0.99618333333333331, 0.99663333333333337, 0.99663333333333337, 0.99661666666666671, 0.99601666666666666, 0.99618333333333331, 0.99673333333333336, 0.9962833333333333, 0.99614999999999998, 0.99655000000000005, 0.99665000000000004, 0.99650000000000005, 0.99631666666666663, 0.99603333333333333, 0.99653333333333338, 0.99665000000000004, 0.99673333333333336, 0.99696666666666667, 0.99661666666666671, 0.99643333333333328, 0.99658333333333338, 0.99634999999999996, 0.99660000000000004, 0.99683333333333335, 0.9966666666666667, 0.99688333333333334, 0.99678333333333335, 0.99650000000000005, 0.99616666666666664, 0.99704999999999999, 0.99729999999999996, 0.99626666666666663, 0.99690000000000001, 0.99704999999999999, 0.99708333333333332, 0.99661666666666671, 0.99668333333333337, 0.997, 0.99698333333333333, 0.99686666666666668, 0.99658333333333338]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data = pd.DataFrame(history.history)\n",
    "nome = 'DoubleConvMNISTNormDropDesloc_t1' + str(datetime.datetime.now())+'.json'\n",
    "data = data.to_json()\n",
    "with open(nome, \"w+\") as output_file:\n",
    "    output_file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
